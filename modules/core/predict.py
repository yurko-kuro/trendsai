# predict.py
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

import sqlite3
from datetime import timedelta

import numpy as np
import pandas as pd
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    GRU,
    Dense,
    Input,
    MultiHeadAttention,
    LayerNormalization,
    Dropout,
)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.losses import Huber

import matplotlib.pyplot as plt
import matplotlib.dates as mdates


# === –¥–æ–ø–æ–º—ñ–∂–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏ ===

def rmse(y_true, y_pred) -> float:
    return float(np.sqrt(mean_squared_error(y_true, y_pred)))


def mape(y_true, y_pred, eps: float = 1e-8) -> float:
    y_true = np.asarray(y_true).reshape(-1)
    y_pred = np.asarray(y_pred).reshape(-1)
    denom = np.clip(np.abs(y_true), eps, None)
    return float(np.mean(np.abs((y_true - y_pred) / denom)) * 100.0)


def smape(y_true, y_pred, eps: float = 1e-8) -> float:
    y_true = np.asarray(y_true).reshape(-1)
    y_pred = np.asarray(y_pred).reshape(-1)
    denom = np.abs(y_true) + np.abs(y_pred) + eps
    return float(np.mean(2.0 * np.abs(y_true - y_pred) / denom) * 100.0)


# === –ø–æ–∑–∏—Ü—ñ–π–Ω–µ –∫–æ–¥—É–≤–∞–Ω–Ω—è ===

class PositionalEncoding(tf.keras.layers.Layer):
    """–õ–µ–≥–∫–µ —Å–∏–Ω—É—Å–æ—ó–¥–∞–ª—å–Ω–µ –ø–æ–∑–∏—Ü—ñ–π–Ω–µ –∫–æ–¥—É–≤–∞–Ω–Ω—è –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π."""
    def __init__(self, d_model: int, **kwargs):
        super().__init__(**kwargs)
        self.d_model = d_model

    def get_config(self):
        config = super().get_config()
        config.update({"d_model": self.d_model})
        return config

    def call(self, x):
        # x: (batch, seq_len, d_model)
        seq_len = tf.shape(x)[1]
        positions = tf.cast(tf.range(seq_len)[:, tf.newaxis], tf.float32)
        dims = tf.cast(tf.range(self.d_model)[tf.newaxis, :], tf.float32)

        angle_rates = 1.0 / tf.pow(
            10000.0,
            (2 * (dims // 2)) / tf.cast(self.d_model, tf.float32),
        )
        angle_rads = positions * angle_rates  # (seq_len, d_model)

        # –ø–∞—Ä–Ω—ñ —ñ–Ω–¥–µ–∫—Å–∏ ‚Äî sin, –Ω–µ–ø–∞—Ä–Ω—ñ ‚Äî cos
        sines = tf.sin(angle_rads[:, 0::2])
        cosines = tf.cos(angle_rads[:, 1::2])

        pos_encoding = tf.concat([sines, cosines], axis=-1)  # (seq_len, d_model)
        pos_encoding = tf.expand_dims(pos_encoding, 0)       # (1, seq_len, d_model)

        return x + pos_encoding


# === –æ—Å–Ω–æ–≤–Ω–∏–π –∫–ª–∞—Å –ø—Ä–æ–≥–Ω–æ–∑—É ===

class GoogleTrendsPredictor:
    def __init__(self, db_path: str = "db.sqlite3", seq_length: int = 12,
                 prediction_days: int = 30):
        self.db_path = db_path
        self.seq_length = seq_length
        self.prediction_days = prediction_days
        self.scaler = RobustScaler()
        self.model = self._build_tgru()

    # === –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –≥—ñ–±—Ä–∏–¥–Ω–æ—ó –º–æ–¥–µ–ª—ñ Transformer‚ÄìGRU ===

    def _build_tgru(self):
        """
        –ì—ñ–±—Ä–∏–¥–Ω–∞ –º–æ–¥–µ–ª—å T-GRU:
        - 1 –±–ª–æ–∫ Transformer-encoder (MultiHeadAttention + FFN + LayerNorm)
        - 2 —à–∞—Ä–∏ GRU (128, 64)
        - Dense(32, relu) + Dense(1)
        """
        d_model = 128  # —Ä–æ–∑–º—ñ—Ä –ø—Ä–æ—Å—Ç–æ—Ä—É –æ–∑–Ω–∞–∫ –ø—ñ—Å–ª—è –ø—Ä–æ—î–∫—Ü—ñ—ó

        inp = Input(shape=(self.seq_length, 1))

        # 1) –õ—ñ–Ω—ñ–π–Ω–∞ –ø—Ä–æ—î–∫—Ü—ñ—è –≤ d_model-–≤–∏–º—ñ—Ä–Ω–∏–π –ø—Ä–æ—Å—Ç—ñ—Ä
        x = Dense(d_model)(inp)

        # 2) –ü–æ–∑–∏—Ü—ñ–π–Ω–µ –∫–æ–¥—É–≤–∞–Ω–Ω—è
        x = PositionalEncoding(d_model)(x)

        # 3) Transformer-encoder block (—Å–ø—Ä–æ—â–µ–Ω–∏–π, 1 —à–∞—Ä)
        attn_out = MultiHeadAttention(
            num_heads=8,
            key_dim=d_model // 8,
        )(x, x)
        attn_out = Dropout(0.1)(attn_out)
        x1 = LayerNormalization(epsilon=1e-6)(x + attn_out)

        ffn = Dense(d_model, activation="relu")(x1)
        ffn = Dense(d_model)(ffn)
        x2 = LayerNormalization(epsilon=1e-6)(x1 + ffn)

        # 4) GRU-—à–∞—Ä–∏
        x_gru = GRU(128, activation="tanh", return_sequences=True)(x2)
        x_gru = GRU(64, activation="tanh")(x_gru)

        # 5) –í–∏—Ö—ñ–¥–Ω—ñ —à–∞—Ä–∏
        x_dense = Dense(32, activation="relu")(x_gru)
        out = Dense(1)(x_dense)

        model = Model(inputs=inp, outputs=out)
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss=Huber(delta=1.0),
        )
        return model

    # === —Ä–æ–±–æ—Ç–∞ –∑ –¥–∞–Ω–∏–º–∏ ===

    def load_data(self, keyword: str) -> pd.DataFrame:
        try:
            with sqlite3.connect(self.db_path) as conn:
                q = """
                SELECT i.date, i.value
                FROM trends_interest i
                JOIN trends_keyword k ON i.keyword_id = k.id
                WHERE k.name = ? AND i.is_partial = 0
                ORDER BY i.date
                """
                return pd.read_sql(q, conn, params=(keyword,), parse_dates=["date"])
        except Exception as e:
            print(f"[DB] –ü–æ–º–∏–ª–∫–∞: {e}")
            return pd.DataFrame()

    def create_sequences(self, arr: np.ndarray):
        X, y = [], []
        for i in range(len(arr) - self.seq_length):
            X.append(arr[i : i + self.seq_length])
            y.append(arr[i + self.seq_length])
        return np.array(X), np.array(y)

    # === –Ω–∞–≤—á–∞–Ω–Ω—è ===

    def train_model(self, X_train, y_train, X_val, y_val, tag: str = "tgru"):
        callbacks = [
            EarlyStopping(
                monitor="val_loss",
                patience=10,
                restore_best_weights=True,
            ),
            ModelCheckpoint(
                f"best_{tag}.keras",
                monitor="val_loss",
                save_best_only=True,
            ),
        ]
        self.model.fit(
            X_train,
            y_train,
            epochs=40,
            batch_size=32,
            validation_data=(X_val, y_val),
            callbacks=callbacks,
            verbose=1,
            shuffle=False,  # –≤–∞–∂–ª–∏–≤–æ –¥–ª—è —Ç–∞–π–º—Å–µ—Ä—ñ–π
        )

    # === –æ—Ü—ñ–Ω–∫–∞ ===

    def evaluate_model(self, y_val_scaled, y_pred_scaled, keyword: str | None = None):
        """
        keyword –∑–∞–ª–∏—à–µ–Ω–æ –æ–ø—Ü—ñ–π–Ω–∏–º –ª–∏—à–µ –¥–ª—è —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ –∑—ñ —Å—Ç–∞—Ä–∏–º pipeline,
        –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è.
        """
        y_pred = self.scaler.inverse_transform(y_pred_scaled)
        y_true = self.scaler.inverse_transform(y_val_scaled)

        metrics = {
            "RMSE": rmse(y_true, y_pred),
            "MAE": float(mean_absolute_error(y_true, y_pred)),
            "R2": float(r2_score(y_true, y_pred)),
            "MAPE": mape(y_true, y_pred),
            "sMAPE": smape(y_true, y_pred),
        }
        print("\nüìä –û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ:", {k: round(v, 4) for k, v in metrics.items()})
        return metrics

    # === –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –º–∞–π–±—É—Ç–Ω—ñ—Ö –∑–Ω–∞—á–µ–Ω—å ===

    def generate_future_predictions(self, last_sequence, days_to_predict: int):
        preds = []
        cur = last_sequence.copy()
        for _ in range(days_to_predict):
            nxt = self.model.predict(cur, verbose=0)
            preds.append(self.scaler.inverse_transform(nxt)[0, 0])
            cur = np.append(cur[:, 1:, :], nxt.reshape(1, 1, 1), axis=1)
        return preds

    # === –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è ===

    def plot_results(self, data, y_pred, future_dates, future_values, keyword: str):
        plt.figure(figsize=(12, 6))
        plt.plot(
            data["date"],
            data["value"],
            label="–§–∞–∫—Ç–∏—á–Ω—ñ –¥–∞–Ω—ñ",
            marker="o",
        )

        split_idx = int(0.8 * len(data))
        val_dates = data["date"][split_idx:]
        m = min(len(val_dates), len(y_pred))
        plt.plot(
            val_dates[:m],
            y_pred[:m],
            label="–ü—Ä–æ–≥–Ω–æ–∑ (–≤–∞–ª—ñ–¥–∞—Ü—ñ—è)",
            linestyle="--",
        )

        plt.plot(
            future_dates,
            future_values,
            label=f"–ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ {len(future_dates)} –¥–Ω—ñ–≤",
            marker="x",
        )

        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m-%d"))
        plt.xticks(rotation=45)
        plt.legend()
        plt.title(f"'{keyword}' ‚Äî –∞–Ω–∞–ª—ñ–∑ —ñ –ø—Ä–æ–≥–Ω–æ–∑")
        plt.xlabel("–î–∞—Ç–∞")
        plt.ylabel("–†–µ–π—Ç–∏–Ω–≥")
        plt.grid(True)
        plt.tight_layout()
        plt.show()

    # === –≥–æ–ª–æ–≤–Ω–∏–π –º–µ—Ç–æ–¥ –ø—Ä–æ–≥–Ω–æ–∑—É (—ñ–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∏–π —Ä–µ–∂–∏–º view) ===

    def forecast(self, keyword: str):
        data = self.load_data(keyword)
        if data.empty:
            print(f"–î–∞–Ω—ñ –¥–ª—è '{keyword}' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
            return None

        scaled = self.scaler.fit_transform(
            data["value"].values.reshape(-1, 1)
        )
        X, y = self.create_sequences(scaled)

        split = int(0.8 * len(X))
        X_train, X_val = X[:split], X[split:]
        y_train, y_val = y[:split], y[split:]

        self.train_model(X_train, y_train, X_val, y_val, tag="tgru")

        y_pred_scaled = self.model.predict(X_val)
        self.evaluate_model(y_val, y_pred_scaled, keyword=keyword)

        last_seq = scaled[-self.seq_length :].reshape(1, self.seq_length, 1)
        future_vals = self.generate_future_predictions(
            last_seq, self.prediction_days
        )
        future_dates = [
            data["date"].max() + timedelta(days=i)
            for i in range(1, self.prediction_days + 1)
        ]

        y_pred = self.scaler.inverse_transform(y_pred_scaled)
        self.plot_results(data, y_pred, future_dates, future_vals, keyword)

        res = pd.DataFrame(
            {
                "–î–∞—Ç–∞": future_dates,
                "–ü—Ä–æ–≥–Ω–æ–∑–æ–≤–∞–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥": future_vals,
            }
        )
        print("\n–¢–∞–±–ª–∏—Ü—è –ø—Ä–æ–≥–Ω–æ–∑—É –ø–æ –¥–Ω—è—Ö:\n", res.to_string(index=False))
        return res
